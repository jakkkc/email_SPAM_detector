{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing modules\n",
    "\n",
    "# data manupulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# text cleaning\n",
    "import string\n",
    "import re\n",
    "# nlp preprosessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43edc341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1252', 'confidence': 0.7257971165545478, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "with open('spam.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "print(result)\n",
    "df = pd.read_csv('spam.csv', encoding=result['encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c5a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='Windows-1252')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6438a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                Will Ì_ b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: v2, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = df['v2']\n",
    "email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e887307",
   "metadata": {},
   "source": [
    "#### Text Cleaning\n",
    "\n",
    "Steps to be followed\n",
    "\n",
    "1. **Completeness** - check at missing values and handle them\n",
    "2. **Consistency** - check for duplicate values\n",
    "3. **Uniformity** - we will rename our columnsas well us cleaning our texts by removing punctuations capital letters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123ad440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Importing the re module for regular expressions\n",
    "\n",
    "class DataCleaning:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def missing_values(self):  # Checking for percentage of missing values in each column\n",
    "        missing_values = self.df.isnull().sum()\n",
    "        missing_percentage = self.df.isnull().mean() * 100\n",
    "        return missing_percentage\n",
    "    \n",
    "    def drop_columns(self, columns):\n",
    "        self.df = self.df.drop(columns=columns, axis=1)\n",
    "        return self.df\n",
    "\n",
    "    def change_col_name(self):  # Changing column names for uniformity\n",
    "      self.df = self.df.rename(columns={'v1': 'email_type', 'v2': 'email'})\n",
    "      return self.df\n",
    "    \n",
    "    def check_duplicates(self):\n",
    "        duplicates = self.df.duplicated().sum()\n",
    "        print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_email(email):  # Removing punctuations, uppercase, white spaces, \n",
    "        if isinstance(email, str):\n",
    "            email = email.lower()\n",
    "            email = re.sub(r'[^\\w\\s]', '', email)\n",
    "            email = re.sub(r'\\s+', ' ', email).strip()\n",
    "            email = re.sub(r'@\\w+', '', email)\n",
    "        return email\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e1e55",
   "metadata": {},
   "source": [
    "**Completeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c393152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1             0.000000\n",
       "v2             0.000000\n",
       "Unnamed: 2    99.102656\n",
       "Unnamed: 3    99.784637\n",
       "Unnamed: 4    99.892319\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = DataCleaning(df)\n",
    "missing_percentages = clean_df.missing_values()\n",
    "missing_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff38c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = DataCleaning(df)\n",
    "df = clean_df.drop_columns(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e054c3",
   "metadata": {},
   "source": [
    "**Consistency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae6569e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 403\n"
     ]
    }
   ],
   "source": [
    "# Now call the check_duplicates method\n",
    "clean_df.check_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c68cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: v1\n",
      "v1\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: v2\n",
      "v2\n",
      "Sorry, I'll call later                                                                                                                                                                                                                                                                                                                       30\n",
      "I cant pick the phone right now. Pls send a message                                                                                                                                                                                                                                                                                          12\n",
      "Ok...                                                                                                                                                                                                                                                                                                                                        10\n",
      "Say this slowly.? GOD,I LOVE YOU &amp; I NEED YOU,CLEAN MY HEART WITH YOUR BLOOD.Send this to Ten special people &amp; u c miracle tomorrow, do it,pls,pls do it...                                                                                                                                                                           4\n",
      "Ok                                                                                                                                                                                                                                                                                                                                            4\n",
      "                                                                                                                                                                                                                                                                                                                                             ..\n",
      "WIN a year supply of CDs 4 a store of ur choice worth å£500 & enter our å£100 Weekly draw txt MUSIC to 87066 Ts&Cs www.Ldew.com.subs16+1win150ppmx3                                                                                                                                                                                           2\n",
      "Ok then i will come to ur home after half an hour                                                                                                                                                                                                                                                                                             2\n",
      "You have 1 new voicemail. Please call 08719181513.                                                                                                                                                                                                                                                                                            2\n",
      "K..k..i'm also fine:)when will you complete the course?                                                                                                                                                                                                                                                                                       2\n",
      "I know you are thinkin malaria. But relax, children cant handle malaria. She would have been worse and its gastroenteritis. If she takes enough to replace her loss her temp will reduce. And if you give her malaria meds now she will just vomit. Its a self limiting illness she has which means in a few days it will completely stop     2\n",
      "Name: count, Length: 281, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_duplicate_values_and_columns(dataframe):\n",
    "    for col in df.columns:\n",
    "      duplicates = df[df.duplicated(subset=[col], keep=False)][col]\n",
    "      if not duplicates.empty:\n",
    "          print(f\"Column: {col}\")\n",
    "          print(duplicates.value_counts())\n",
    "          print()\n",
    "\n",
    "\n",
    "duplicate = print_duplicate_values_and_columns(df)\n",
    "\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d558883f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping the duplicate values\n",
    "df.drop_duplicates()\n",
    "#confirming duplicates dropped\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea023b80",
   "metadata": {},
   "source": [
    "**Uniformity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e505ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # changing columns names by calling the class method\n",
    "df = clean_df.change_col_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5275ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['email_type'] = df['email_type'].replace({'ham':\n",
    "                         'Not spam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6e725a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go until jurong point crazy available only in ...\n",
       "1                              ok lar joking wif u oni\n",
       "2    free entry in 2 a wkly comp to win fa cup fina...\n",
       "3          u dun say so early hor u c already then say\n",
       "4    nah i dont think he goes to usf he lives aroun...\n",
       "Name: cleaned_email, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing text cleaning  by removing @, punctuation ,uppercase by hep of our class method\n",
    "df['cleaned_email'] = df['email'].apply(DataCleaning.clean_email)\n",
    "df['cleaned_email'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd092cf",
   "metadata": {},
   "source": [
    "**Text processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c65beb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextProcessing:\n",
    "    def __init__(self, df, column_name):\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "        self.sw = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        # Tokenize the text in the specified column\n",
    "        self.df['tokenized'] = self.df[self.column_name].apply(word_tokenize)\n",
    "        return self.df\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        # Remove stopwords from the tokenized text\n",
    "        self.df['no_stopwords'] = self.df['tokenized'].apply(\n",
    "            lambda x: [word for word in x if word not in self.sw]\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    def lemmatize_text(self):\n",
    "        # Lemmatize the text\n",
    "        self.df['lemmatized'] = self.df['no_stopwords'].apply(\n",
    "            lambda x: [self.lemmatizer.lemmatize(word) for word in x]\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "\n",
    "\n",
    "text_processor = TextProcessing(df, 'cleaned_email')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a09491",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f54a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, crazy, available, o...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "df = text_processor.tokenize_text()\n",
    "df['tokenized'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ece13e",
   "metadata": {},
   "source": [
    "**Removing stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f42e83ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, jurong, point, crazy, available, bugis, n...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
       "3        [u, dun, say, early, hor, u, c, already, say]\n",
       "4    [nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: no_stopwords, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = text_processor.remove_stopwords()\n",
    "df['no_stopwords'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e772f581",
   "metadata": {},
   "source": [
    "**Normallization**\n",
    "\n",
    "Using **Lemmatization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c82ad353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [go, jurong, point, crazy, available, bugis, n...\n",
       "1                          [ok, lar, joking, wif, u, oni]\n",
       "2       [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
       "3           [u, dun, say, early, hor, u, c, already, say]\n",
       "4       [nah, dont, think, go, usf, life, around, though]\n",
       "                              ...                        \n",
       "5567    [2nd, time, tried, 2, contact, u, u, å750, pou...\n",
       "5568                  [ì_, b, going, esplanade, fr, home]\n",
       "5569                      [pity, mood, soany, suggestion]\n",
       "5570    [guy, bitching, acted, like, id, interested, b...\n",
       "5571                                   [rofl, true, name]\n",
       "Name: lemmatized, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = text_processor.lemmatize_text()\n",
    "df['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8325cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_type</th>\n",
       "      <th>email</th>\n",
       "      <th>cleaned_email</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not spam</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not spam</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not spam</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not spam</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[this, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, å750, pou...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, å750, pou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>Not spam</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>will ì_ b going to esplanade fr home</td>\n",
       "      <td>[will, ì_, b, going, to, esplanade, fr, home]</td>\n",
       "      <td>[ì_, b, going, esplanade, fr, home]</td>\n",
       "      <td>[ì_, b, going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>Not spam</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity was in mood for that soany other suggestions</td>\n",
       "      <td>[pity, was, in, mood, for, that, soany, other,...</td>\n",
       "      <td>[pity, mood, soany, suggestions]</td>\n",
       "      <td>[pity, mood, soany, suggestion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>Not spam</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "      <td>[the, guy, did, some, bitching, but, i, acted,...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Not spam</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "      <td>[rofl, its, true, to, its, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     email_type                                              email  \\\n",
       "0      Not spam  Go until jurong point, crazy.. Available only ...   \n",
       "1      Not spam                      Ok lar... Joking wif u oni...   \n",
       "2          spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      Not spam  U dun say so early hor... U c already then say...   \n",
       "4      Not spam  Nah I don't think he goes to usf, he lives aro...   \n",
       "...         ...                                                ...   \n",
       "5567       spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   Not spam              Will Ì_ b going to esplanade fr home?   \n",
       "5569   Not spam  Pity, * was in mood for that. So...any other s...   \n",
       "5570   Not spam  The guy did some bitching but I acted like i'd...   \n",
       "5571   Not spam                         Rofl. Its true to its name   \n",
       "\n",
       "                                          cleaned_email  \\\n",
       "0     go until jurong point crazy available only in ...   \n",
       "1                               ok lar joking wif u oni   \n",
       "2     free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3           u dun say so early hor u c already then say   \n",
       "4     nah i dont think he goes to usf he lives aroun...   \n",
       "...                                                 ...   \n",
       "5567  this is the 2nd time we have tried 2 contact u...   \n",
       "5568               will ì_ b going to esplanade fr home   \n",
       "5569  pity was in mood for that soany other suggestions   \n",
       "5570  the guy did some bitching but i acted like id ...   \n",
       "5571                          rofl its true to its name   \n",
       "\n",
       "                                              tokenized  \\\n",
       "0     [go, until, jurong, point, crazy, available, o...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3     [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4     [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "...                                                 ...   \n",
       "5567  [this, is, the, 2nd, time, we, have, tried, 2,...   \n",
       "5568      [will, ì_, b, going, to, esplanade, fr, home]   \n",
       "5569  [pity, was, in, mood, for, that, soany, other,...   \n",
       "5570  [the, guy, did, some, bitching, but, i, acted,...   \n",
       "5571                   [rofl, its, true, to, its, name]   \n",
       "\n",
       "                                           no_stopwords  \\\n",
       "0     [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3         [u, dun, say, early, hor, u, c, already, say]   \n",
       "4     [nah, dont, think, goes, usf, lives, around, t...   \n",
       "...                                                 ...   \n",
       "5567  [2nd, time, tried, 2, contact, u, u, å750, pou...   \n",
       "5568                [ì_, b, going, esplanade, fr, home]   \n",
       "5569                   [pity, mood, soany, suggestions]   \n",
       "5570  [guy, bitching, acted, like, id, interested, b...   \n",
       "5571                                 [rofl, true, name]   \n",
       "\n",
       "                                             lemmatized  \n",
       "0     [go, jurong, point, crazy, available, bugis, n...  \n",
       "1                        [ok, lar, joking, wif, u, oni]  \n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3         [u, dun, say, early, hor, u, c, already, say]  \n",
       "4     [nah, dont, think, go, usf, life, around, though]  \n",
       "...                                                 ...  \n",
       "5567  [2nd, time, tried, 2, contact, u, u, å750, pou...  \n",
       "5568                [ì_, b, going, esplanade, fr, home]  \n",
       "5569                    [pity, mood, soany, suggestion]  \n",
       "5570  [guy, bitching, acted, like, id, interested, b...  \n",
       "5571                                 [rofl, true, name]  \n",
       "\n",
       "[5572 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54588ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joinging the lemmatized email\n",
    "df['lemmatized'] = df['lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d1275c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
